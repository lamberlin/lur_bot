{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "f9adfe44-3f34-4824-826e-7fd776a4aaf5",
    "_uuid": "325c3042-e63e-4c79-8491-974d39b0d90d",
    "execution": {
     "iopub.execute_input": "2023-11-17T13:46:34.486070Z",
     "iopub.status.busy": "2023-11-17T13:46:34.485786Z",
     "iopub.status.idle": "2023-11-17T13:46:48.925676Z",
     "shell.execute_reply": "2023-11-17T13:46:48.924806Z",
     "shell.execute_reply.started": "2023-11-17T13:46:34.486045Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(text, tokenizer):\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    return [w for w in words if w not in stop_words]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def text_to_bert_embedding(text, model, tokenizer):\n",
    "    model = model.to(device)\n",
    "    try:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        input_ids = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        embedding = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        return embedding\n",
    "    except:\n",
    "        return [[]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1c469370-7c72-42da-9d6d-5e6a9f1035a9",
    "_uuid": "e7be5178-603e-4295-8261-aa60f3c29d1c",
    "execution": {
     "iopub.execute_input": "2023-11-17T13:46:48.927825Z",
     "iopub.status.busy": "2023-11-17T13:46:48.927490Z",
     "iopub.status.idle": "2023-11-17T13:46:52.708032Z",
     "shell.execute_reply": "2023-11-17T13:46:52.706371Z",
     "shell.execute_reply.started": "2023-11-17T13:46:48.927792Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(r\"./major.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['major_field'] != 'education']\n",
    "df = df[df['major_field'] != 'law']\n",
    "df = df[df['major_field'] != 'civil.engineering']\n",
    "df = df[df['major_field'] != 'geography']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['linguistics', 'economics', 'history', 'architecture_art',\n",
       "       'electrical & comp. engineering', 'mechanical.engineering',\n",
       "       'psychology', 'english.language.and.literature',\n",
       "       'mathematics_statistic', 'sociology', 'biology', 'medicine_health',\n",
       "       'archeology', 'chemical.engineering', 'development.studies',\n",
       "       'political.science', 'accounting.and.finance', 'philosophy',\n",
       "       'physics', 'agriculture_environment', 'communication_info'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['major_field'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12875, 7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "min_data_per_university = df.groupby('University').size().min()\n",
    "\n",
    "# Create an empty list to store the sampled data\n",
    "sampled_data = []\n",
    "\n",
    "# Iterate over unique universities\n",
    "for university in df['University'].unique():\n",
    "    # Get the data for the current university\n",
    "    university_data = df[df['University'] == university]\n",
    "    \n",
    "    # If the current university has more data than 'min_data_per_university', sample 'min_data_per_university' rows\n",
    "    if len(university_data) > min_data_per_university:\n",
    "        sampled_university_data = university_data.sample(min_data_per_university, random_state=42)\n",
    "    else:\n",
    "        sampled_university_data = university_data\n",
    "    \n",
    "    sampled_data.append(sampled_university_data)\n",
    "balanced_df = pd.concat(sampled_data)\n",
    "\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.1, random_state=42)\n",
    "\n",
    "test_df.to_csv('major_test.csv', index=False)\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 12875/12875 [24:30<00:00,  8.75it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "for item in train_df[\"reviews_lemmatized\"].apply(lambda x: x.split()):\n",
    "    tmp.extend(item)\n",
    "users_stop_words = []\n",
    "for _ in Counter(tmp).most_common(30):\n",
    "    if _[0] in stopwords.words('english'):\n",
    "        continue\n",
    "    else:\n",
    "        users_stop_words.append(_[0])\n",
    "stop_words = set(stopwords.words('english')+users_stop_words)\n",
    "\n",
    "with open('stop_words.pkl', 'wb') as file:\n",
    "    pickle.dump(stop_words, file)\n",
    "\n",
    "with open('stop_words.pkl', 'rb') as file:\n",
    "    stop_words = pickle.load(file)\n",
    "tqdm.pandas()\n",
    "embeddings_2 = train_df[\"reviews_lemmatized\"].progress_apply(lambda x: text_to_bert_embedding(preprocess(x, tokenizer), model, tokenizer))\n",
    "data_to_save = {\n",
    "    'embeddings': np.vstack(embeddings_2),\n",
    "    'reviews':  train_df[\"reviews_lemmatized\"].to_numpy(),\n",
    "    \"University\":  train_df[\"University\"].to_numpy(),\n",
    "    \"Professor\":train_df[\"Professor\"].to_numpy(),\n",
    "    \"major_field\": train_df[\"major_field\"].to_numpy(),\n",
    "    \"course\": train_df[\"course\"].to_numpy(),\n",
    "    \"created\": train_df[\"created\"].to_numpy(),\n",
    "}\n",
    "\n",
    "np.save('major_embedding_bert.npy', data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chemical.engineering'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "embedding_file='major_embedding_bert.zip'\n",
    "def load_reviews(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"extracted_data\")\n",
    "        npy_file = [f for f in os.listdir(\"extracted_data\") if f.endswith('.npy')][0]\n",
    "        npy_file_path = os.path.join(\"extracted_data\", npy_file)\n",
    "        data = np.load(npy_file_path, allow_pickle=True).item()\n",
    "        os.remove(npy_file_path)\n",
    "        os.rmdir(\"extracted_data\")\n",
    "    return data['embeddings'], data['reviews'], data['University'], data['Professor'],data['major_field'], data['course'], data['created']\n",
    "\n",
    "\n",
    "def calculate_topic_probabilities_with_reviews(input_text, model, tokenizer, zip_file_path):\n",
    "    embeddings, reviews, universities, professor, sub_topics, course, created_dates = load_reviews(zip_file_path)\n",
    "    input_text_tokens = preprocess(input_text, tokenizer)\n",
    "    input_text_embedding = text_to_bert_embedding(\" \".join(input_text_tokens), model, tokenizer)\n",
    "    similarities = cosine_similarity(input_text_embedding.reshape(1, -1), embeddings).flatten()\n",
    "\n",
    "    topic_scores = {}\n",
    "    topic_counts = {}\n",
    "    \n",
    "    for score, topic in zip(similarities, sub_topics):\n",
    "        topic_scores[topic] = topic_scores.get(topic, 0) + score\n",
    "        topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
    "    for topic in topic_scores:\n",
    "        topic_scores[topic] /= topic_counts[topic]\n",
    "\n",
    "    top_topics = sorted(topic_scores, key=topic_scores.get, reverse=True)[:5]\n",
    "    total_topic_score = sum(topic_scores[topic] for topic in top_topics)\n",
    "    topic_probabilities = {topic: topic_scores[topic] / total_topic_score for topic in top_topics}\n",
    "\n",
    "    filtered_indices = [i for i, topic in enumerate(sub_topics) if topic in top_topics]\n",
    "    filtered_data = {\n",
    "        'major_field': [sub_topics[i] for i in filtered_indices],\n",
    "        'review': [reviews[i] for i in filtered_indices],\n",
    "        'similarity_score': [similarities[i] for i in filtered_indices],\n",
    "        'University': [universities[i] for i in filtered_indices],\n",
    "        'professor': [professor[i] for i in filtered_indices],\n",
    "        'course': [course[i] for i in filtered_indices],\n",
    "        'created': [created_dates[i] for i in filtered_indices],\n",
    "        'topic_probability': [topic_probabilities[sub_topics[i]] for i in filtered_indices]\n",
    "    }\n",
    "\n",
    "    filtered_df = pd.DataFrame(filtered_data)\n",
    "\n",
    "    return filtered_df\n",
    "def evaluate_major(input_text, model, tokenizer, zip_file_path, input_major):\n",
    "    df = calculate_topic_probabilities_with_reviews(input_text, model, tokenizer, zip_file_path)\n",
    "    majors_prob_df = df.groupby('major_field')['topic_probability'].mean()\n",
    "    sorted_majors = majors_prob_df.sort_values(ascending=False)\n",
    "    ranking = None\n",
    "    if input_major in sorted_majors.index:\n",
    "        ranking = sorted_majors.index.tolist().index(input_major) + 1\n",
    "    if ranking is None:\n",
    "        assessment = 'bad'\n",
    "    elif ranking <= 3:\n",
    "        assessment = 'perfect'\n",
    "    elif ranking <= 5:\n",
    "        assessment = 'good'\n",
    "    elif ranking <= 10:\n",
    "        assessment = 'reasonable'\n",
    "    else:\n",
    "        assessment = 'bad'\n",
    "    top_majors_list = sorted_majors.head(5).index.tolist()\n",
    "    return assessment, top_majors_list\n",
    "input_text = \"I love working professor Matthew Potts, He is funny and knowledgeble\"\n",
    "\n",
    "a,b=evaluate_major(input_text, model, tokenizer, embedding_file,'chemical.engineering')\n",
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University</th>\n",
       "      <th>WAS</th>\n",
       "      <th>highest_prob_major</th>\n",
       "      <th>Most_Relevant_Review</th>\n",
       "      <th>Most_Relevant_faculty</th>\n",
       "      <th>Most_Relevant_course</th>\n",
       "      <th>Most_Relevant_Created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston University</td>\n",
       "      <td>0.045704</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>hey, hey, here's something ... y'all ever hear...</td>\n",
       "      <td>Rosina Georgiadis</td>\n",
       "      <td>CH109</td>\n",
       "      <td>2005-12-13 12:55:07 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brown University</td>\n",
       "      <td>0.045170</td>\n",
       "      <td>communication_info</td>\n",
       "      <td>stay away. lorin doesn't seem to care about te...</td>\n",
       "      <td>Lorin Crawford</td>\n",
       "      <td>PHP2605</td>\n",
       "      <td>2020-01-28 07:12:26 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carnegie Mellon University</td>\n",
       "      <td>0.044022</td>\n",
       "      <td>biology</td>\n",
       "      <td>seems nice, but doesn't seem like a great teac...</td>\n",
       "      <td>Linda Robic</td>\n",
       "      <td>3121</td>\n",
       "      <td>2014-11-21 23:53:42 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>0.041757</td>\n",
       "      <td>biology</td>\n",
       "      <td>40% of your grade will be based on blog posts ...</td>\n",
       "      <td>Lili Yamasaki</td>\n",
       "      <td>GU4300</td>\n",
       "      <td>2021-03-24 18:11:44 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cornell University</td>\n",
       "      <td>0.045517</td>\n",
       "      <td>biology</td>\n",
       "      <td>take this class!! mrs calliaud is literally th...</td>\n",
       "      <td>Marina Caillaud</td>\n",
       "      <td>BSOC2100</td>\n",
       "      <td>2023-09-26 23:52:39 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Duke University</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>english.language.and.literature</td>\n",
       "      <td>a bit of a stickler on grading papers but she ...</td>\n",
       "      <td>Nancy Mullenneaux</td>\n",
       "      <td>WRIT20</td>\n",
       "      <td>2011-11-22 14:06:31 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harvard University</td>\n",
       "      <td>0.043033</td>\n",
       "      <td>sociology</td>\n",
       "      <td>garfinkle, funny name, funny professor.don't k...</td>\n",
       "      <td>Paul Garfinkle</td>\n",
       "      <td>WS499</td>\n",
       "      <td>2006-09-08 23:25:03 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Johns Hopkins University</td>\n",
       "      <td>0.043773</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>i don't think the class should be more credits...</td>\n",
       "      <td>Louise Pasternack</td>\n",
       "      <td>CHEMLAB</td>\n",
       "      <td>2007-06-12 12:16:01 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>0.034145</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>i agree so much w/the comment about patronizin...</td>\n",
       "      <td>Janet Schrenk</td>\n",
       "      <td>5111</td>\n",
       "      <td>2004-06-13 20:54:18 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New York University</td>\n",
       "      <td>0.045498</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>yoel is an incredible ta. his recitations are ...</td>\n",
       "      <td>Yoel Ohayon</td>\n",
       "      <td>GENCHEM126</td>\n",
       "      <td>2017-05-29 16:01:34 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Northwestern University</td>\n",
       "      <td>0.044279</td>\n",
       "      <td>biology</td>\n",
       "      <td>quite possibly the worst teacher i have ever h...</td>\n",
       "      <td>Erinn Mee</td>\n",
       "      <td>BIO210</td>\n",
       "      <td>2007-12-12 00:08:30 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Princeton University</td>\n",
       "      <td>0.035226</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>genius yes, but of what benefit is a professor...</td>\n",
       "      <td>Maitland Jones</td>\n",
       "      <td>CHM301</td>\n",
       "      <td>2004-07-09 00:27:30 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stanford University</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>english.language.and.literature</td>\n",
       "      <td>good poet with an amazing archive of quotation...</td>\n",
       "      <td>Ken Fields</td>\n",
       "      <td>RELPOET</td>\n",
       "      <td>2007-04-02 22:13:31 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>University of California, Berkeley</td>\n",
       "      <td>0.045407</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>really entertaining but the downside is that h...</td>\n",
       "      <td>Luciano Moretto</td>\n",
       "      <td>CHEM4A</td>\n",
       "      <td>2006-12-18 22:41:29 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>University of California, Los Angeles</td>\n",
       "      <td>0.044792</td>\n",
       "      <td>english.language.and.literature</td>\n",
       "      <td>not an affective teacher: there was no lecture...</td>\n",
       "      <td>Christopher Mott</td>\n",
       "      <td>ENG173B</td>\n",
       "      <td>2010-08-17 18:20:24 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>University of California, San Diego</td>\n",
       "      <td>0.044680</td>\n",
       "      <td>biology</td>\n",
       "      <td>i loved having dr. reuther for bild 4. he real...</td>\n",
       "      <td>Keefe Reuther</td>\n",
       "      <td>BILD4</td>\n",
       "      <td>2021-01-04 00:34:54 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>University of California, Santa Barbara</td>\n",
       "      <td>0.045431</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>run run run! im very good at chem but he expla...</td>\n",
       "      <td>Thomas Hooker</td>\n",
       "      <td>CHEM1B</td>\n",
       "      <td>2004-08-28 14:49:58 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>University of North Carolina at Chapel Hill</td>\n",
       "      <td>0.046048</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>i passed this class but i thought the &amp;quot;po...</td>\n",
       "      <td>Lisa Volaric</td>\n",
       "      <td>HBHJBB</td>\n",
       "      <td>2010-04-12 04:27:14 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>University of Pennsylvania</td>\n",
       "      <td>0.044581</td>\n",
       "      <td>communication_info</td>\n",
       "      <td>sexy class!!! sexy prof!! the class was about ...</td>\n",
       "      <td>Carolyn Marvin</td>\n",
       "      <td>COMM666</td>\n",
       "      <td>2007-01-03 00:14:09 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>University of Southern California</td>\n",
       "      <td>0.045156</td>\n",
       "      <td>communication_info</td>\n",
       "      <td>i am obsessed w pierson. her lectures make pot...</td>\n",
       "      <td>Jillian Pierson</td>\n",
       "      <td>COMM200</td>\n",
       "      <td>2022-04-18 05:00:38 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>University of Texas at Austin</td>\n",
       "      <td>0.046183</td>\n",
       "      <td>english.language.and.literature</td>\n",
       "      <td>this coming spring semester will be natalia's ...</td>\n",
       "      <td>Natalia Sylvester</td>\n",
       "      <td>CRW325F</td>\n",
       "      <td>2021-01-12 02:16:30 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>University of Washington</td>\n",
       "      <td>0.045522</td>\n",
       "      <td>biology</td>\n",
       "      <td>i love ben!!!! after the terrible teacher toby...</td>\n",
       "      <td>Ben Kerr</td>\n",
       "      <td>BIOL180</td>\n",
       "      <td>2007-10-25 00:25:03 +0000 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Yale University</td>\n",
       "      <td>0.044448</td>\n",
       "      <td>chemical.engineering</td>\n",
       "      <td>the clacss is hard because it is chemistry but...</td>\n",
       "      <td>Patrick Loria</td>\n",
       "      <td>CHEM556</td>\n",
       "      <td>2008-08-06 11:40:51 +0000 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     University       WAS   \n",
       "0                             Boston University  0.045704  \\\n",
       "1                              Brown University  0.045170   \n",
       "2                    Carnegie Mellon University  0.044022   \n",
       "3                           Columbia University  0.041757   \n",
       "4                            Cornell University  0.045517   \n",
       "5                               Duke University  0.036281   \n",
       "6                            Harvard University  0.043033   \n",
       "7                      Johns Hopkins University  0.043773   \n",
       "8         Massachusetts Institute of Technology  0.034145   \n",
       "9                           New York University  0.045498   \n",
       "10                      Northwestern University  0.044279   \n",
       "11                         Princeton University  0.035226   \n",
       "12                          Stanford University  0.043347   \n",
       "13           University of California, Berkeley  0.045407   \n",
       "14        University of California, Los Angeles  0.044792   \n",
       "15          University of California, San Diego  0.044680   \n",
       "16      University of California, Santa Barbara  0.045431   \n",
       "17  University of North Carolina at Chapel Hill  0.046048   \n",
       "18                   University of Pennsylvania  0.044581   \n",
       "19            University of Southern California  0.045156   \n",
       "20                University of Texas at Austin  0.046183   \n",
       "21                     University of Washington  0.045522   \n",
       "22                              Yale University  0.044448   \n",
       "\n",
       "                 highest_prob_major   \n",
       "0              chemical.engineering  \\\n",
       "1                communication_info   \n",
       "2                           biology   \n",
       "3                           biology   \n",
       "4                           biology   \n",
       "5   english.language.and.literature   \n",
       "6                         sociology   \n",
       "7              chemical.engineering   \n",
       "8              chemical.engineering   \n",
       "9              chemical.engineering   \n",
       "10                          biology   \n",
       "11             chemical.engineering   \n",
       "12  english.language.and.literature   \n",
       "13             chemical.engineering   \n",
       "14  english.language.and.literature   \n",
       "15                          biology   \n",
       "16             chemical.engineering   \n",
       "17             chemical.engineering   \n",
       "18               communication_info   \n",
       "19               communication_info   \n",
       "20  english.language.and.literature   \n",
       "21                          biology   \n",
       "22             chemical.engineering   \n",
       "\n",
       "                                 Most_Relevant_Review Most_Relevant_faculty   \n",
       "0   hey, hey, here's something ... y'all ever hear...     Rosina Georgiadis  \\\n",
       "1   stay away. lorin doesn't seem to care about te...        Lorin Crawford   \n",
       "2   seems nice, but doesn't seem like a great teac...           Linda Robic   \n",
       "3   40% of your grade will be based on blog posts ...         Lili Yamasaki   \n",
       "4   take this class!! mrs calliaud is literally th...       Marina Caillaud   \n",
       "5   a bit of a stickler on grading papers but she ...     Nancy Mullenneaux   \n",
       "6   garfinkle, funny name, funny professor.don't k...        Paul Garfinkle   \n",
       "7   i don't think the class should be more credits...     Louise Pasternack   \n",
       "8   i agree so much w/the comment about patronizin...         Janet Schrenk   \n",
       "9   yoel is an incredible ta. his recitations are ...           Yoel Ohayon   \n",
       "10  quite possibly the worst teacher i have ever h...             Erinn Mee   \n",
       "11  genius yes, but of what benefit is a professor...        Maitland Jones   \n",
       "12  good poet with an amazing archive of quotation...            Ken Fields   \n",
       "13  really entertaining but the downside is that h...       Luciano Moretto   \n",
       "14  not an affective teacher: there was no lecture...      Christopher Mott   \n",
       "15  i loved having dr. reuther for bild 4. he real...         Keefe Reuther   \n",
       "16  run run run! im very good at chem but he expla...         Thomas Hooker   \n",
       "17  i passed this class but i thought the &quot;po...          Lisa Volaric   \n",
       "18  sexy class!!! sexy prof!! the class was about ...        Carolyn Marvin   \n",
       "19  i am obsessed w pierson. her lectures make pot...       Jillian Pierson   \n",
       "20  this coming spring semester will be natalia's ...     Natalia Sylvester   \n",
       "21  i love ben!!!! after the terrible teacher toby...              Ben Kerr   \n",
       "22  the clacss is hard because it is chemistry but...         Patrick Loria   \n",
       "\n",
       "   Most_Relevant_course          Most_Relevant_Created  \n",
       "0                 CH109  2005-12-13 12:55:07 +0000 UTC  \n",
       "1               PHP2605  2020-01-28 07:12:26 +0000 UTC  \n",
       "2                  3121  2014-11-21 23:53:42 +0000 UTC  \n",
       "3                GU4300  2021-03-24 18:11:44 +0000 UTC  \n",
       "4              BSOC2100  2023-09-26 23:52:39 +0000 UTC  \n",
       "5                WRIT20  2011-11-22 14:06:31 +0000 UTC  \n",
       "6                 WS499  2006-09-08 23:25:03 +0000 UTC  \n",
       "7               CHEMLAB  2007-06-12 12:16:01 +0000 UTC  \n",
       "8                  5111  2004-06-13 20:54:18 +0000 UTC  \n",
       "9            GENCHEM126  2017-05-29 16:01:34 +0000 UTC  \n",
       "10               BIO210  2007-12-12 00:08:30 +0000 UTC  \n",
       "11               CHM301  2004-07-09 00:27:30 +0000 UTC  \n",
       "12              RELPOET  2007-04-02 22:13:31 +0000 UTC  \n",
       "13               CHEM4A  2006-12-18 22:41:29 +0000 UTC  \n",
       "14              ENG173B  2010-08-17 18:20:24 +0000 UTC  \n",
       "15                BILD4  2021-01-04 00:34:54 +0000 UTC  \n",
       "16               CHEM1B  2004-08-28 14:49:58 +0000 UTC  \n",
       "17               HBHJBB  2010-04-12 04:27:14 +0000 UTC  \n",
       "18              COMM666  2007-01-03 00:14:09 +0000 UTC  \n",
       "19              COMM200  2022-04-18 05:00:38 +0000 UTC  \n",
       "20              CRW325F  2021-01-12 02:16:30 +0000 UTC  \n",
       "21              BIOL180  2007-10-25 00:25:03 +0000 UTC  \n",
       "22              CHEM556  2008-08-06 11:40:51 +0000 UTC  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_weighted_average_similarity(input_text, model, tokenizer, embedding_file):\n",
    "    filtered_df = calculate_topic_probabilities_with_reviews(input_text, model, tokenizer, embedding_file)\n",
    "    \n",
    "    filtered_df['University'] = filtered_df['University'].astype(str)\n",
    "    filtered_df['major_field'] = filtered_df['major_field'].astype(str)\n",
    "\n",
    "    mean_scores = filtered_df.groupby(['University', 'major_field']).agg({'similarity_score': 'mean'}).reset_index()\n",
    "    mean_scores['weighted_score'] = mean_scores.apply(\n",
    "        lambda x: x['similarity_score'] * filtered_df[filtered_df['major_field'] == x['major_field']]['topic_probability'].iloc[0], \n",
    "        axis=1\n",
    "    )\n",
    "    was_scores = mean_scores.groupby('University')['weighted_score'].sum().reset_index().rename(columns={'weighted_score': 'WAS'})\n",
    "\n",
    "    total_was = was_scores['WAS'].sum()\n",
    "    was_scores['WAS'] = was_scores['WAS'] / total_was\n",
    "\n",
    "    highest_prob_topic = mean_scores.loc[mean_scores.groupby('University')['similarity_score'].idxmax()][['University', 'major_field']]\n",
    "    highest_prob_topic.columns = ['University', 'highest_prob_major']\n",
    "    highest_prob_topic['University'] = highest_prob_topic['University'].astype(str)\n",
    "    highest_prob_topic['highest_prob_major'] = highest_prob_topic['highest_prob_major'].astype(str)\n",
    "\n",
    "    result = pd.merge(was_scores, highest_prob_topic, on='University')\n",
    "\n",
    "    relevant_review = filtered_df.loc[filtered_df.groupby(['University', 'major_field'])['similarity_score'].idxmax()][['University', 'major_field', 'review','professor', 'course', 'created']]\n",
    "    relevant_review.columns = ['University', 'highest_prob_major', 'Most_Relevant_Review', 'Most_Relevant_faculty', 'Most_Relevant_course', 'Most_Relevant_Created']\n",
    "    relevant_review['University'] = relevant_review['University'].astype(str)\n",
    "    relevant_review['highest_prob_major'] = relevant_review['highest_prob_major'].astype(str)\n",
    "\n",
    "    final_result = pd.merge(result, relevant_review, on=['University', 'highest_prob_major'])\n",
    "\n",
    "    return final_result[['University', 'WAS', 'highest_prob_major', 'Most_Relevant_Review','Most_Relevant_faculty', 'Most_Relevant_course', 'Most_Relevant_Created']]\n",
    "\n",
    "def calculate_confidence_score(input_text, model, tokenizer, embedding_file):\n",
    "    df = calculate_topic_probabilities_with_reviews(input_text, model, tokenizer, embedding_file)    \n",
    "    topic_counts = df['major_field'].value_counts()\n",
    "    total_data_points = len(df)\n",
    "    topic_portions = topic_counts / total_data_points\n",
    "    topic_probabilities = df['topic_probability'].groupby(df['major_field']).mean()\n",
    "    weighted_average = sum(topic_portions * topic_probabilities)\n",
    "    return weighted_average\n",
    "\n",
    "\n",
    "input_text = \"I love working professor Matthew Potts, He is funny and knowledgeble\"\n",
    "calculate_weighted_average_similarity(input_text, model, tokenizer, embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.200118808712514"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_confidence_score(input_text, model, tokenizer, embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reviews: 100%|████████████████████████████████████████████████████████████| 489/489 [05:52<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48261758691206547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_df= pd.read_csv(r\"test.csv\", encoding='ISO-8859-1')\n",
    "predicted_universities = []\n",
    "for review in tqdm(test_df['reviews_lemmatized'], desc=\"Processing Reviews\"):\n",
    "    preprocessed_review = \" \".join(review.lower().split())\n",
    "    result = calculate_weighted_average_similarity(preprocessed_review, model, tokenizer, embedding_file)\n",
    "    top_universities = result.nlargest(5, 'WAS')['University'].tolist()\n",
    "    predicted_universities.append(top_universities)\n",
    "test_df['Predicted_University'] = predicted_universities\n",
    "test_df['Match'] = test_df.apply(lambda row: row['University'] in row['Predicted_University'], axis=1)\n",
    "accuracy = test_df['Match'].sum() / len(test_df)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed6af1ff-1f38-4947-a929-aa7f50e58be4",
    "_uuid": "8ae27590-0a1d-451b-ac7d-7e2fa2784a60"
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinol\\anaconda3\\envs\\py39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'course' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m ta \u001b[38;5;241m=\u001b[39m EmbeddingsCalculator()\n\u001b[0;32m      5\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love working professor Matthew Potts, He is funny and knowledgeble\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_topic_probabilities_with_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\留学\\ucb\\其他\\工作实习\\Internship\\Reach Best\\model\\major review model\\major_embeddings.py:71\u001b[0m, in \u001b[0;36mEmbeddingsCalculator.calculate_topic_probabilities_with_reviews\u001b[1;34m(self, input_text)\u001b[0m\n\u001b[0;32m     62\u001b[0m topic_probabilities \u001b[38;5;241m=\u001b[39m {topic: topic_scores[topic] \u001b[38;5;241m/\u001b[39m total_topic_score \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m top_topics}\n\u001b[0;32m     64\u001b[0m filtered_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sub_topics) \u001b[38;5;28;01mif\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m top_topics]\n\u001b[0;32m     65\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor_field\u001b[39m\u001b[38;5;124m'\u001b[39m: [sub_topics[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m: [reviews[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_score\u001b[39m\u001b[38;5;124m'\u001b[39m: [similarities[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;124m'\u001b[39m: [universities[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofessor\u001b[39m\u001b[38;5;124m'\u001b[39m: [professors[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourse\u001b[39m\u001b[38;5;124m'\u001b[39m: [course[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m'\u001b[39m: [created_dates[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_probability\u001b[39m\u001b[38;5;124m'\u001b[39m: [topic_probabilities[sub_topics[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices]\n\u001b[0;32m     74\u001b[0m }\n\u001b[0;32m     76\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(filtered_data)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_df\n",
      "File \u001b[1;32mD:\\留学\\ucb\\其他\\工作实习\\Internship\\Reach Best\\model\\major review model\\major_embeddings.py:71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     62\u001b[0m topic_probabilities \u001b[38;5;241m=\u001b[39m {topic: topic_scores[topic] \u001b[38;5;241m/\u001b[39m total_topic_score \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m top_topics}\n\u001b[0;32m     64\u001b[0m filtered_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sub_topics) \u001b[38;5;28;01mif\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m top_topics]\n\u001b[0;32m     65\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor_field\u001b[39m\u001b[38;5;124m'\u001b[39m: [sub_topics[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m: [reviews[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_score\u001b[39m\u001b[38;5;124m'\u001b[39m: [similarities[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;124m'\u001b[39m: [universities[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofessor\u001b[39m\u001b[38;5;124m'\u001b[39m: [professors[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourse\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[43mcourse\u001b[49m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m'\u001b[39m: [created_dates[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices],\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_probability\u001b[39m\u001b[38;5;124m'\u001b[39m: [topic_probabilities[sub_topics[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices]\n\u001b[0;32m     74\u001b[0m }\n\u001b[0;32m     76\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(filtered_data)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'course' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from major_embeddings import EmbeddingsCalculator\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ta = EmbeddingsCalculator()\n",
    "input_text = \"I love working professor Matthew Potts, He is funny and knowledgeble\"\n",
    "ta.calculate_topic_probabilities_with_reviews(input_text)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1936563,
     "sourceId": 6674905,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3969385,
     "sourceId": 6911685,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3989128,
     "sourceId": 6945917,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3994594,
     "sourceId": 6954877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
